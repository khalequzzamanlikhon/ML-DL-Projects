{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     col_0   col_1 col_2 col_3  col_4 col_5 col_6\n",
      "500     A1      B0    C2   D58    100    E1    F1\n",
      "501     A0      B0    C2    D0      0    E0    F2\n",
      "502     A0      B0   C11    D1    100    E1    F2\n",
      "503     A0      B0    C4   D71    100    E1    F2\n",
      "504     A2      B0    C9    D1      0    E1    F2\n",
      "...    ...     ...   ...   ...    ...   ...   ...\n",
      "2623    A1      B0    C2    D1      0    E1    F2\n",
      "2624    A0   B0       C8    D1      0    E1    F2\n",
      "2625    A0      BO    C7    D1    100    E1    F2\n",
      "2626    A0      B0   C11    D1    100    E1    F2\n",
      "2627    A0      B0    C2    D1    100    E1    F2\n",
      "\n",
      "[2128 rows x 7 columns]\n",
      "col_0    0\n",
      "col_1    0\n",
      "col_2    0\n",
      "col_3    0\n",
      "col_4    0\n",
      "col_5    0\n",
      "col_6    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission_y = sample_submission.drop(columns=[\"UID\"])\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_data.iloc[500:, 1:-1]\n",
    "y_train = train_data.iloc[500:, -1]\n",
    "X_val = train_data.iloc[:500, 1:-1]\n",
    "y_val = train_data.iloc[:500, -1]\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "\n",
    "# Categorical columns\n",
    "categorical_cols = ['col_0', 'col_1', 'col_2', 'col_3', 'col_5', 'col_6']\n",
    "\n",
    "# # Fill NaN values in categorical columns with 'Unknown'\n",
    "# def fill_na(data, categorical_cols):\n",
    "#     for col in categorical_cols:\n",
    "#         data[col].fillna('Unknown', inplace=True)\n",
    "#     return data\n",
    "\n",
    "# Fill NaN values in categorical columns with the most frequent value in the respective columns\n",
    "def fill_na(data, categorical_cols):\n",
    "    for col in categorical_cols:\n",
    "        most_frequent_value = data[col].mode().iloc[0]  # Get the mode (most frequent value)\n",
    "        data[col].fillna(most_frequent_value, inplace=True)\n",
    "    return data\n",
    "\n",
    "X_train = fill_na(X_train, categorical_cols)\n",
    "X_val = fill_na(X_val, categorical_cols)\n",
    "X_test = fill_na(X_test, categorical_cols)\n",
    "\n",
    "print(X_train)\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "# label encoding\n",
    "def label_encoding(data, categorical_cols):\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        unique_categories = data[col].unique()\n",
    "        encoding_map = {category: index + 1 for index, category in enumerate(unique_categories)}\n",
    "        data[col] = data[col].map(encoding_map)\n",
    "        label_encoders[col] = encoding_map\n",
    "    return data\n",
    "\n",
    "X_train = label_encoding(X_train, categorical_cols)\n",
    "X_val = label_encoding(X_val, categorical_cols)\n",
    "X_test = label_encoding(X_test, categorical_cols)\n",
    "\n",
    "# normalize\n",
    "def normalize(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "X_train = normalize(X_train)\n",
    "X_val = normalize(X_val)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "# convert df into numpy\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "sample_submission_y = sample_submission_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999, Loss: 929598.7805\n",
      "Epoch 1999, Loss: 929598.7592\n",
      "Epoch 2999, Loss: 929598.7592\n",
      "Early stopping at epoch 3260 with best validation RMSE: 62781.8045\n"
     ]
    }
   ],
   "source": [
    "# Neural network architecture\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32\n",
    "output_size = 1\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01   \n",
    "epochs = 60000\n",
    "lambda_reg = 0.01  # L2 regularization strength\n",
    "\n",
    "# Initialize weights and biases\n",
    "\n",
    "w1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / (input_size + hidden_size))\n",
    "w2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / (hidden_size + output_size))\n",
    "# w1 = np.random.randn(input_size, hidden_size) - 0.5\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "# w2 = np.random.randn(hidden_size, output_size) - 0.5\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Prediction function\n",
    "def predict(X, w1, b1, w2, b2):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    predicted_output = z2\n",
    "    return predicted_output\n",
    "\n",
    "# RMSE function\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    mean_squared_error = np.mean(squared_errors)\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 60\n",
    "best_val_rmse = float(\"inf\")\n",
    "best_weights = None\n",
    "best_epoch = 0\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X_train, w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    \n",
    "    # loss(MSE with L2 regularization)\n",
    "    loss = (0.5 / X_train.shape[0]) * np.mean((z2 - y_train.reshape(-1, 1)) ** 2)\n",
    "    loss += (lambda_reg / (2 * X_train.shape[0])) * (np.sum(w1 ** 2) + np.sum(w2 ** 2))\n",
    "\n",
    "    # Backpropagation\n",
    "    output_error = z2 - y_train.reshape(-1, 1)\n",
    "    d_z2 = output_error / X_train.shape[0]\n",
    "\n",
    "    hidden_layer_error = np.dot(d_z2, w2.T)\n",
    "    d_hidden_layer = hidden_layer_error * (z1 > 0)\n",
    "\n",
    "    # Update weights and biases\n",
    "    w2 -= learning_rate * np.dot(a1.T, d_z2) + lambda_reg * w2\n",
    "    b2 -= learning_rate * np.sum(d_z2, axis=0, keepdims=True)\n",
    "    w1 -= learning_rate * np.dot(X_train.T, d_hidden_layer) + lambda_reg * w1\n",
    "    b1 -= learning_rate * np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "    # Print loss for monitoring\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    # Check validation RMSE for early stopping\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        val_predictions = predict(X_val, w1, b1, w2, b2)\n",
    "        val_rmse = calculate_rmse(y_val, val_predictions)\n",
    "\n",
    "        # If validation RMSE is the best so far, update the best RMSE and weights\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_weights = (w1.copy(), b1.copy(), w2.copy(), b2.copy())\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        # If there's no improvement for 'patience' epochs, stop training\n",
    "        if epoch + 1 - best_epoch >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} with best validation RMSE: {best_val_rmse:.4f}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 62899.700468136485\n",
      "val rmse 62781.80448634513\n",
      "test rmse 144949.47073465976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the best weights for prediction\n",
    "best_w1, best_b1, best_w2, best_b2 = best_weights\n",
    "\n",
    "#prediction and rmse on train set\n",
    "train_predict=predict(X_train,best_w1,best_b1,best_w2,best_b2)\n",
    "train_rmse=calculate_rmse(train_predict,y_train)\n",
    "\n",
    "#prediction and rmse on train set\n",
    "val_predict=predict(X_val,best_w1,best_b1,best_w2,best_b2)\n",
    "val_rmse=calculate_rmse(val_predict,y_val)\n",
    "\n",
    "#prediction and rmse on test set\n",
    "test_predict=predict(X_test,best_w1,best_b1,best_w2,best_b2)\n",
    "test_rmse=calculate_rmse(test_predict,sample_submission_y)\n",
    "print(\"train rmse\",train_rmse)\n",
    "print(\"val rmse\",val_rmse)\n",
    "print(\"test rmse\",test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicitions on test set using best weights and biases\n",
    "#best_predictions = predict(X_test, best_w1, best_b1, best_w2, best_b2)\n",
    "result_df = pd.DataFrame({'UID': test_data['UID'], 'y': test_predict.flatten()})\n",
    "# Save predictions\n",
    "result_df.to_csv('nn_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
